{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7fd1455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5542c",
   "metadata": {},
   "source": [
    "### Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4b1a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filename):\n",
    "    '''Load dataset from the specified filename, clean it by dropping duplicates, filtering for Singapore.'''\n",
    "    df = pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "    # Replace unicode\n",
    "    df = df.replace('\\u202f', ' ', regex=True).replace('\\u2013', '-', regex=True)\n",
    "\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(subset=['address'], inplace=True)\n",
    "\n",
    "    # Filter out rows where country is not in SG\n",
    "    df = df[df['complete_address'].astype(str).str.contains('\"country\":\"SG\"', na=False)]\n",
    "\n",
    "    # Drop unused columns\n",
    "    df = drop_columns(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_missing_open_hours(df):\n",
    "    '''Filter out rows where open_hours is not available.'''\n",
    "    return df[df['open_hours'].astype(str) != \"{}\"]\n",
    "\n",
    "def drop_low_ratings(df, threshold=1):\n",
    "    '''Filter out rows where ratings are below a certain threshold.'''\n",
    "    return df[df['rating'].astype(float) >= threshold]\n",
    "\n",
    "def drop_columns(df):\n",
    "    '''Drop unused columns from the dataframe.'''\n",
    "    df.drop(columns=[\n",
    "        'input_id', \n",
    "        'popular_times', \n",
    "        'plus_code',\n",
    "        # 'reviews_per_rating',\n",
    "        'cid',\n",
    "        'status',\n",
    "        'reviews_link',\n",
    "        'thumbnail',\n",
    "        'timezone',\n",
    "        'data_id',\n",
    "        'reservations',\n",
    "        'order_online',\n",
    "        'menu',\n",
    "        'owner',\n",
    "        # 'address',\n",
    "        'user_reviews',\n",
    "        'user_reviews_extended',\n",
    "        'emails',\n",
    "        ], inplace=True, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def combine_dataframes(dfs):\n",
    "    '''Combine multiple dataframes into one, dropping duplicates based on the 'address' column.'''\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    num_duplicates = combined['address'].duplicated().sum()\n",
    "    combined = combined.drop_duplicates(subset=['address'])\n",
    "    print(f\"duplicate rows: {num_duplicates}\")\n",
    "    return combined\n",
    "\n",
    "def print_unique_categories(df, exclude_keyword=None):\n",
    "    '''\n",
    "    Print unique categories from the dataframe.\n",
    "    Optionally exclude categories containing one or more keywords.\n",
    "    '''\n",
    "    unique_categories = df['category'].unique()\n",
    "\n",
    "    if exclude_keyword:\n",
    "        exclude_keyword = [kw.lower() for kw in exclude_keyword]\n",
    "        unique_categories = [\n",
    "            c for c in unique_categories\n",
    "            if all(kw not in str(c).lower() for kw in exclude_keyword)\n",
    "        ]\n",
    "\n",
    "    print(\"Unique categories:\")\n",
    "    for category in unique_categories:\n",
    "        print(f\" - {category}\")\n",
    "\n",
    "def to_csv(df, filename):\n",
    "    '''Save the dataframe to a CSV file.'''\n",
    "    if not df.empty:\n",
    "        df.to_csv(filename, index=False)\n",
    "    else:\n",
    "        print(f\"No data to save to {os.path.basename(filename)}.\")\n",
    "\n",
    "def drop_low_ratings(df, threshold=1):\n",
    "    '''Filter out rows where ratings are below a certain threshold.'''\n",
    "    return df[df['review_rating'].astype(float) >= threshold]\n",
    "\n",
    "def drop_low_reviews(df, threshold=100):\n",
    "    '''Filter out rows where number of reviews are below a certain threshold.'''\n",
    "    return df[df['review_count'].astype(float) >= threshold]\n",
    "\n",
    "def remove_street_view(images):\n",
    "    if not isinstance(images, (list, str)):\n",
    "        return images\n",
    "\n",
    "    try:\n",
    "        image_json = json.loads(images)\n",
    "        filtered_images = [img for img in image_json if not any(keyword in img.get('title', '').lower() for keyword in ['street view', '360'])]\n",
    "        return filtered_images\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return images\n",
    "    \n",
    "def map_price(price):\n",
    "    if pd.isna(price):\n",
    "        return None\n",
    "\n",
    "    price = str(price).strip()\n",
    "\n",
    "    # Direct symbolic mapping\n",
    "    symbol_map = {'$': 1, '$$': 2, '$$$': 3, '$$$$': 4}\n",
    "    if price in symbol_map:\n",
    "        return symbol_map[price]\n",
    "\n",
    "    # Extract numbers\n",
    "    nums = re.findall(r'\\d+', price)\n",
    "    if not nums:\n",
    "        return None\n",
    "\n",
    "    nums = [int(n) for n in nums]\n",
    "    mid = sum(nums) / len(nums)\n",
    "\n",
    "    # Determine price level\n",
    "    if mid < 20:\n",
    "        return 1\n",
    "    elif 20 <= mid <= 50:\n",
    "        return 2\n",
    "    elif 50 < mid <= 100:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "    \n",
    "def clean_images_field(images_raw):\n",
    "    \"\"\"Extract all image URLs from the images field.\"\"\"\n",
    "    if isinstance(images_raw, list):\n",
    "        # When it's already a list of dicts\n",
    "        return [\n",
    "            item[\"image\"]\n",
    "            for item in images_raw\n",
    "            if isinstance(item, dict) and \"image\" in item and isinstance(item[\"image\"], str)\n",
    "        ]\n",
    "    return []\n",
    "    \n",
    "def clean_videos_field(videos_raw):\n",
    "    \"\"\"Clean and normalize video URLs.\"\"\"\n",
    "    if not isinstance(videos_raw, str) or not videos_raw.strip():\n",
    "        return []\n",
    "    urls = re.findall(r'https://[^,\\s]+', videos_raw)\n",
    "    cleaned = []\n",
    "    for url in urls:\n",
    "        url = url.split(\"|\")[0]           # remove resolution part\n",
    "        if \"=mm\" in url:\n",
    "            url = url.split(\"=mm\")[0]     # remove anything after =mm\n",
    "        cleaned.append(url)\n",
    "    return cleaned\n",
    "\n",
    "def create_flags(row, keywords):\n",
    "    \"\"\"Return True if any of the given keywords appear in option names with enabled=True.\"\"\"\n",
    "    if pd.isna(row):\n",
    "        return False\n",
    "\n",
    "    data = json.loads(row) if isinstance(row, str) else row\n",
    "\n",
    "    for cat in data:\n",
    "        for opt in cat.get('options', []):\n",
    "            opt_name = opt.get('name', '').lower()\n",
    "            enabled = opt.get('enabled', False)\n",
    "            if enabled and any(kw.lower() in opt_name for kw in keywords):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def remove_about(row, category_name):\n",
    "    '''Remove the category name from the about field if it appears at the start.'''\n",
    "    if pd.isna(row) or pd.isna(category_name):\n",
    "        return row\n",
    "\n",
    "    data = json.loads(row) if isinstance(row, str) else row\n",
    "    filtered = [cat for cat in data if cat.get('name') != category_name]\n",
    "    return json.dumps(filtered) if isinstance(row, str) else filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2609db1",
   "metadata": {},
   "source": [
    "### Concatenate all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5b6b815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate rows: 1935\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = '../sg/'\n",
    "OUTPUT_DIR = '../output/'\n",
    "\n",
    "# Read all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(INPUT_DIR, \"*.csv\"))\n",
    "\n",
    "# Clean and combine data\n",
    "dataframes = [clean_data(file) for file in csv_files]\n",
    "all_data = combine_dataframes(dataframes)\n",
    "\n",
    "# print(all_data.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aa468ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "all_data = all_data.rename(columns={\"title\": \"name\", \"raw_images\": \"videos\"})\n",
    "all_data['images'] = all_data['images'].apply(remove_street_view)\n",
    "all_data['price_range'] = all_data['price_range'].apply(map_price)\n",
    "all_data = all_data.rename(columns={\"price_range\": \"price_level\"})\n",
    "all_data['complete_address'] = all_data['complete_address'].str.replace('\\\\u0026', \"&\", regex=False)\n",
    "\n",
    "# Handle images & videos\n",
    "all_data['images'] = all_data['images'].apply(clean_images_field)\n",
    "all_data['videos'] = all_data['videos'].apply(clean_videos_field)\n",
    "\n",
    "# Drop low rating rows\n",
    "all_data = drop_low_ratings(all_data, threshold=1)\n",
    "\n",
    "# Create flags for kids & pets friendly\n",
    "all_data['kids_friendly'] = all_data['about'].apply(create_flags, keywords=['Good for kids'])\n",
    "all_data['dogs_friendly'] = all_data['about'].apply(create_flags, keywords=['Dogs allowed', 'Dogs allowed inside', 'Dogs allowed outside'])\n",
    "all_data['wheelchair_rental'] = all_data['about'].apply(create_flags, keywords=['Wheelchair rental'])\n",
    "all_data['wheelchair_accessible_car_park'] = all_data['about'].apply(create_flags, keywords=['Wheelchair accessible car park'])\n",
    "all_data['wheelchair_accessible_entrance'] = all_data['about'].apply(create_flags, keywords=['Wheelchair accessible entrance'])\n",
    "all_data['wheelchair_accessible_seating'] = all_data['about'].apply(create_flags, keywords=['Wheelchair accessible seating'])\n",
    "all_data['wheelchair_accessible_toilet'] = all_data['about'].apply(create_flags, keywords=['Wheelchair accessible toilet'])\n",
    "all_data['halal_food'] = all_data['about'].apply(create_flags, keywords=['Halal food'])\n",
    "all_data['vegan_options'] = all_data['about'].apply(create_flags, keywords=['Vegan options'])\n",
    "all_data['vegetarian_options'] = all_data['about'].apply(create_flags, keywords=['Vegetarian options'])\n",
    "all_data['reservations_required'] = all_data['about'].apply(create_flags, keywords=['Reservations required'])\n",
    "all_data['hiking'] = all_data['about'].apply(create_flags, keywords=['Hiking', 'Point-to-point trail', 'Trail difficulty'])\n",
    "all_data['cycling'] = all_data['about'].apply(create_flags, keywords=['Cycling'])\n",
    "\n",
    "# Remove certain \"about\" field\n",
    "all_data['about'] = all_data['about'].apply(remove_about, category_name=\"Atmosphere\")\n",
    "all_data['about'] = all_data['about'].apply(remove_about, category_name=\"Amenities\")\n",
    "all_data['about'] = all_data['about'].apply(remove_about, category_name=\"Dining options\")\n",
    "all_data['about'] = all_data['about'].apply(remove_about, category_name=\"From the business\")\n",
    "all_data['about'] = all_data['about'].apply(remove_about, category_name=\"Getting here\")\n",
    "all_data['about'] = all_data['about'].apply(remove_about, category_name=\"Offerings\")\n",
    "all_data['about'] = all_data['about'].apply(remove_about, category_name=\"Parking\")\n",
    "all_data['about'] = all_data['about'].apply(remove_about, category_name=\"Payments\")\n",
    "all_data['about'] = all_data['about'].apply(remove_about, category_name=\"Pets\")\n",
    "all_data['about'] = all_data['about'].apply(remove_about, category_name=\"Popular for\")\n",
    "all_data['about'] = all_data['about'].apply(remove_about, category_name=\"Recycling\")\n",
    "all_data['about'] = all_data['about'].apply(remove_about, category_name=\"Service options\")\n",
    "\n",
    "to_csv(all_data, f\"{OUTPUT_DIR}poi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad5f5bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessibility:\n",
      "  - Assistive hearing loop\n",
      "  - Wheelchair rental\n",
      "  - Wheelchair-accessible car park\n",
      "  - Wheelchair-accessible entrance\n",
      "  - Wheelchair-accessible seating\n",
      "  - Wheelchair-accessible toilet\n",
      "\n",
      "Activities:\n",
      "  - Birdwatching\n",
      "  - Cycling\n",
      "  - Hiking\n",
      "  - Jogging\n",
      "  - Point-to-point trail\n",
      "  - Trail difficulty\n",
      "  - Walking\n",
      "\n",
      "Children:\n",
      "  - Discounts for kids\n",
      "  - Family discount\n",
      "  - Good for kids\n",
      "  - Has changing table(s)\n",
      "  - Kid-friendly activities\n",
      "  - Nursing room\n",
      "\n",
      "Crowd:\n",
      "  - Family friendly\n",
      "  - Groups\n",
      "  - LGBTQ+ friendly\n",
      "  - Tourists\n",
      "  - Transgender safe space\n",
      "  - University students\n",
      "\n",
      "Highlights:\n",
      "  - Fireplace\n",
      "  - Great beer selection\n",
      "  - Great cocktails\n",
      "  - Great coffee\n",
      "  - Great dessert\n",
      "  - Great tea selection\n",
      "  - Great wine list\n",
      "  - Rooftop seating\n",
      "  - Sport\n",
      "\n",
      "Planning:\n",
      "  - Accepts reservations\n",
      "  - Brunch reservations recommended\n",
      "  - Dinner reservations recommended\n",
      "  - Lunch reservations recommended\n",
      "  - Reservations required\n",
      "  - Usually a wait\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = {}\n",
    "\n",
    "for row in all_data['about'].dropna():\n",
    "    data = json.loads(row) if isinstance(row, str) else row\n",
    "\n",
    "    for category in data:\n",
    "        cat_name = category.get('name')\n",
    "        opts = [opt.get('name') for opt in category.get('options', [])]\n",
    "\n",
    "        # keep the one with more options if duplicate category appears\n",
    "        if cat_name not in categories or len(opts) > len(categories[cat_name]):\n",
    "            categories[cat_name] = opts\n",
    "    \n",
    "# print results\n",
    "for cat_name, opts in sorted(categories.items()):\n",
    "    print(f\"{cat_name}:\")\n",
    "    for opt in sorted(opts):\n",
    "        print(f\"  - {opt}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f1d7a",
   "metadata": {},
   "source": [
    "### Michelin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a3265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Fill empty price_level using Michelin data\n",
    "michelin = pd.read_csv(f\"{OUTPUT_DIR}michelin.csv\")\n",
    "michelin['price'] = michelin['price'].apply(map_price)\n",
    "price_lookup = michelin.set_index(\"name\")[\"price\"]\n",
    "all_data[\"price_level\"] = all_data[\"price_level\"].fillna(all_data[\"name\"].map(price_lookup))\n",
    "\n",
    "# Fill empty description with Michelin data\n",
    "description_lookup = michelin.set_index(\"name\")[\"description\"]\n",
    "all_data[\"descriptions\"] = all_data[\"descriptions\"].fillna(all_data[\"name\"].map(description_lookup))\n",
    "\n",
    "# Insert michelin images into images field by adding into the list\n",
    "\n",
    "def to_list(s):\n",
    "    if isinstance(s, list):\n",
    "        return s\n",
    "    if not s or not isinstance(s, str):\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# Combine image lists and remove duplicates\n",
    "def merge_images(row):\n",
    "    imgs_all = to_list(row['images'])\n",
    "    imgs_michelin = to_list(row['michelin_images'])\n",
    "    combined = list(dict.fromkeys(imgs_all + imgs_michelin))  # preserves order, removes duplicates\n",
    "    return str(combined)\n",
    "\n",
    "# Assuming `michelin['images']` aligns by index\n",
    "all_data['michelin_images'] = michelin['images']\n",
    "all_data['images'] = all_data.apply(merge_images, axis=1)\n",
    "all_data.drop(columns=['michelin_images'], inplace=True)\n",
    "\n",
    "to_csv(all_data, f\"{OUTPUT_DIR}poi.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
