{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fd1455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import glob\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "os.chdir('/home/kahgin/fika-prep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5542c",
   "metadata": {},
   "source": [
    "### Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86f2af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SLUG_RX = re.compile(r\"[^a-z0-9]+\")\n",
    "\n",
    "def norm_token(s: str) -> str:\n",
    "    s = s.strip().lower().replace(\"&\", \" and \")\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s = _SLUG_RX.sub(\"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "def categories_to_tokens(val):\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      - comma string: \"Learning center, Açaí shop, Science museum\"\n",
    "      - JSON list string: '[\"Learning center\",\"Açaí shop\"]'\n",
    "      - Python list: [\"Learning center\",\"Açaí shop\"]\n",
    "      - None/NaN\n",
    "    Returns list[str] of normalized tokens:\n",
    "      [\"learning_center\",\"acai_shop\",\"science_museum\"]\n",
    "    \"\"\"\n",
    "    if val is None or (isinstance(val, float) and pd.isna(val)):\n",
    "        return []\n",
    "    if isinstance(val, list):\n",
    "        items = [str(x) for x in val if str(x).strip()]\n",
    "    elif isinstance(val, str):\n",
    "        val = val.strip()\n",
    "        try:\n",
    "            parsed = json.loads(val)\n",
    "            if isinstance(parsed, list):\n",
    "                items = [str(x) for x in parsed if str(x).strip()]\n",
    "            else:\n",
    "                items = [p.strip() for p in val.split(\",\") if p.strip()]\n",
    "        except Exception:\n",
    "            items = [p.strip() for p in val.split(\",\") if p.strip()]\n",
    "    else:\n",
    "        items = [str(val).strip()] if str(val).strip() else []\n",
    "    out = []\n",
    "    for x in items:\n",
    "        t = norm_token(x)\n",
    "        if t:\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "def normalize_categories_column(df: pd.DataFrame, src=\"categories\", dst=\"categories\"):\n",
    "    \"\"\"\n",
    "    Converts df[src] to a JSON array string of normalized tokens in df[dst].\n",
    "    Example: \"Açaí shop, Science museum\" -> '[\"acai_shop\",\"science_museum\"]'\n",
    "    \"\"\"\n",
    "    tokens = df[src].apply(categories_to_tokens)\n",
    "    df[dst] = tokens.apply(json.dumps)\n",
    "    return df\n",
    "\n",
    "def categories_json_to_list(val) -> list[str]:\n",
    "    \"\"\"\n",
    "    Safe reader for post-clean_data categories column.\n",
    "    Input is expected to be a JSON list string like '[\"acai_shop\",\"science_museum\"]'.\n",
    "    \"\"\"\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            v = json.loads(val)\n",
    "            if isinstance(v, list):\n",
    "                return v\n",
    "        except Exception:\n",
    "            pass\n",
    "    return []\n",
    "\n",
    "def clean_data(filename):\n",
    "    \"\"\"\n",
    "    Load CSV -> replace some unicode -> dedup by name -> keep SG rows ->\n",
    "    rename title->name -> normalize categories -> drop unused columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "    # Replace common unicode variants\n",
    "    df = (df.replace('\\u202f', ' ', regex=True)\n",
    "            .replace('\\u2013', '-', regex=True)\n",
    "            .replace('\\u0026', '&', regex=True))\n",
    "\n",
    "    # Deduplicate by name\n",
    "    df.drop_duplicates(subset=['name'], inplace=True)\n",
    "\n",
    "    # Keep Singapore rows only\n",
    "    df = df[df['complete_address'].astype(str).str.contains('\"country\":\"SG\"', na=False)]\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={'title': 'name'}, inplace=True)\n",
    "\n",
    "    # Normalize categories to JSON array string of tokens\n",
    "    df = normalize_categories_column(df, src='categories', dst='categories')\n",
    "\n",
    "    # Drop unused columns\n",
    "    df = drop_columns(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_missing_open_hours(df):\n",
    "    \"\"\"Filter rows where open_hours is non-empty object.\"\"\"\n",
    "    return df[df['open_hours'].astype(str) != \"{}\"]\n",
    "\n",
    "def drop_low_ratings(df, threshold=1.0):\n",
    "    \"\"\"Filter rows where review_rating >= threshold.\"\"\"\n",
    "    return df[df['review_rating'].astype(float) >= threshold]\n",
    "\n",
    "def drop_low_reviews(df, threshold=5):\n",
    "    \"\"\"Filter rows where review_count >= threshold.\"\"\"\n",
    "    return df[df['review_count'].astype(float) >= threshold]\n",
    "\n",
    "def drop_columns(df):\n",
    "    \"\"\"Drop unused columns from the dataframe.\"\"\"\n",
    "    df.drop(columns=[\n",
    "        'input_id',\n",
    "        'popular_times',\n",
    "        'plus_code',\n",
    "        'videos',\n",
    "        'reviews_per_rating',\n",
    "        'cid',\n",
    "        'status',\n",
    "        'reviews_link',\n",
    "        'thumbnail',\n",
    "        # 'timezone',\n",
    "        'data_id',\n",
    "        'reservations',\n",
    "        'order_online',\n",
    "        'menu',\n",
    "        'owner',\n",
    "        # 'complete_address',\n",
    "        'user_reviews',\n",
    "        'user_reviews_extended',\n",
    "        'emails',\n",
    "    ], inplace=True, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def combine_dataframes(dfs):\n",
    "    \"\"\"Concat dataframes, dedup by name, print dup count.\"\"\"\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    num_duplicates = combined['name'].duplicated().sum()\n",
    "    combined = combined.drop_duplicates(subset=['name'])\n",
    "    print(f\"duplicate rows: {num_duplicates}\")\n",
    "    return combined\n",
    "\n",
    "def save_categories(df, exclude_keyword=None, filename='../text/categories.txt'):\n",
    "    tokens = []\n",
    "    for val in df['categories'].dropna():\n",
    "        tokens.extend(categories_json_to_list(val))\n",
    "\n",
    "    unique = sorted(set(tokens))\n",
    "\n",
    "    if exclude_keyword:\n",
    "        ex_kw = [kw.lower() for kw in exclude_keyword]\n",
    "        unique = [c for c in unique if all(kw not in c.lower() for kw in ex_kw)]\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for category in unique:\n",
    "            f.write(f\"{category}\\n\")\n",
    "\n",
    "\n",
    "def update_flag_by_options(row, flag_col, keywords, about_col='about'):\n",
    "    \"\"\"Update boolean flag based on 'about' JSON options.\"\"\"\n",
    "\n",
    "    flag_val = row.get(flag_col, False)\n",
    "    if flag_val:\n",
    "        return True\n",
    "\n",
    "    about_data = row.get(about_col)\n",
    "\n",
    "    if pd.isna(about_data):\n",
    "        return flag_val\n",
    "\n",
    "    if isinstance(about_data, str):\n",
    "        try:\n",
    "            about_data = json.loads(about_data)\n",
    "        except json.JSONDecodeError:\n",
    "            return flag_val\n",
    "\n",
    "    keywords = [kw.lower() for kw in keywords]\n",
    "\n",
    "    for cat in about_data:\n",
    "        for opt in cat.get('options', []):\n",
    "            if not opt.get('enabled', False):\n",
    "                continue\n",
    "            opt_name = opt.get('name', '').lower()\n",
    "            for kw in keywords:\n",
    "                if kw in opt_name:\n",
    "                    return True\n",
    "\n",
    "    return flag_val\n",
    "\n",
    "def update_flag_by_categories(row: dict, flag_col: str, target_categories: list[str]):\n",
    "    \"\"\"\n",
    "    Sets a boolean flag to True if any target category token is present.\n",
    "    Expects row['categories'] to be a JSON-array string.\n",
    "    \"\"\"\n",
    "    # keep prior True\n",
    "    if row.get(flag_col, False):\n",
    "        return True\n",
    "\n",
    "    cats = set(categories_json_to_list(row.get(\"categories\")))\n",
    "    targets = {norm_token(t) for t in target_categories}\n",
    "    return len(cats & targets) > 0\n",
    "\n",
    "def filter_exclude_categories(df, exclude_file='../text/exclude.txt'):\n",
    "    \"\"\"\n",
    "    Remove excluded categories based on a file list.\n",
    "    Operates on normalized token list; drops row if it becomes empty (unless it contains 'tourist_attraction').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(exclude_file, 'r', encoding='utf-8') as f:\n",
    "            exclude = {norm_token(line) for line in f if line.strip()}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {exclude_file} not found. Returning original DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    if not exclude:\n",
    "        print(\"Warning: No exclude categories found. Returning original DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    def _filter_tokens(val):\n",
    "        toks = categories_json_to_list(val)\n",
    "        if \"tourist_attraction\" in toks:\n",
    "            return json.dumps(toks)\n",
    "        kept = [t for t in toks if t not in exclude]\n",
    "        return json.dumps(kept) if kept else None\n",
    "\n",
    "    df['categories'] = df['categories'].apply(_filter_tokens)\n",
    "    df = df[df['categories'].notna()].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def remove_street_view(images):\n",
    "    \"\"\"\n",
    "    If 'images' is a JSON string list of dicts: filter out street view.\n",
    "    Otherwise return as-is.\n",
    "    \"\"\"\n",
    "    if not isinstance(images, (list, str)):\n",
    "        return images\n",
    "\n",
    "    try:\n",
    "        image_json = json.loads(images) if isinstance(images, str) else images\n",
    "        filtered_images = [\n",
    "            img for img in image_json\n",
    "            if not any(kw in str(img.get('title', '')).lower() for kw in ['street view', '360'])\n",
    "            and not any(kw in str(img.get('image', '')).lower() for kw in ['streetview'])\n",
    "        ]\n",
    "        return filtered_images\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return images\n",
    "\n",
    "def map_price(price):\n",
    "    \"\"\"Map price strings/symbols to 1..4.\"\"\"\n",
    "    if pd.isna(price):\n",
    "        return None\n",
    "    price = str(price).strip()\n",
    "    symbol_map = {'$': 1, '$$': 2, '$$$': 3, '$$$$': 4}\n",
    "    if price in symbol_map:\n",
    "        return symbol_map[price]\n",
    "    nums = re.findall(r'\\d+', price)\n",
    "    if not nums:\n",
    "        return None\n",
    "    nums = [int(n) for n in nums]\n",
    "    mid = sum(nums) / len(nums)\n",
    "    if mid < 20: return 1\n",
    "    if mid <= 50: return 2\n",
    "    if mid <= 100: return 3\n",
    "    return 4\n",
    "\n",
    "def deprioritize_category(row, keyword):\n",
    "    toks = categories_json_to_list(row.get(\"categories\"))\n",
    "    key = norm_token(keyword)\n",
    "    if key not in toks:\n",
    "        return json.dumps(toks)\n",
    "    toks = [t for t in toks if t != key]\n",
    "    toks.append(key)\n",
    "    return json.dumps(toks)\n",
    "\n",
    "def clean_images_field(images_raw):\n",
    "    \"\"\"Extract all image URLs from images field if it's a list of dicts.\"\"\"\n",
    "    if isinstance(images_raw, list):\n",
    "        return [\n",
    "            item[\"image\"]\n",
    "            for item in images_raw\n",
    "            if isinstance(item, dict) and \"image\" in item and isinstance(item[\"image\"], str)\n",
    "        ]\n",
    "    return []\n",
    "\n",
    "def clean_videos_field(videos_raw):\n",
    "    \"\"\"Extract and clean video URLs from a string.\"\"\"\n",
    "    if not isinstance(videos_raw, str) or not videos_raw.strip():\n",
    "        return []\n",
    "    urls = re.findall(r'https://[^,\\s]+', videos_raw)\n",
    "    cleaned = []\n",
    "    for url in urls:\n",
    "        url = url.split(\"|\")[0]\n",
    "        if \"=mm\" in url:\n",
    "            url = url.split(\"=mm\")[0]\n",
    "        cleaned.append(url)\n",
    "    return cleaned\n",
    "\n",
    "def remove_about(row, category_name):\n",
    "    \"\"\"Remove a category block by name from 'about' JSON.\"\"\"\n",
    "    if pd.isna(row) or pd.isna(category_name):\n",
    "        return row\n",
    "    data = json.loads(row) if isinstance(row, str) else row\n",
    "    filtered = [cat for cat in data if cat.get('name') != category_name]\n",
    "    return json.dumps(filtered) if isinstance(row, str) else filtered\n",
    "\n",
    "def change_resolution(link, scale=3):\n",
    "    \"\"\"Scale Google image URL width/height by 'scale' if matches pattern.\"\"\"\n",
    "    pattern = r'(=w)(\\d+)(-h)(\\d+)(-k-no)$'\n",
    "    replacement = lambda m: f\"=w{int(m.group(2)) * scale}-h{int(m.group(4)) * scale}-k-no\"\n",
    "    return re.sub(pattern, replacement, link)\n",
    "\n",
    "def change_image_resolutions(image_list):\n",
    "    \"\"\"Apply change_resolution to each link in list.\"\"\"\n",
    "    if not isinstance(image_list, list):\n",
    "        return image_list\n",
    "    return [change_resolution(link) for link in image_list if isinstance(link, str)]\n",
    "\n",
    "def save_about_field(df, filename=\"../text/about_field.txt\"):\n",
    "    \"\"\"Group 'about' enabled option names by category display name (raw, not tokens).\"\"\"\n",
    "    grouped = {}\n",
    "    for about_data in df[\"about\"].dropna():\n",
    "        if isinstance(about_data, str):\n",
    "            try:\n",
    "                about_data = json.loads(about_data)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        for cat in about_data:\n",
    "            cname = cat.get(\"name\")\n",
    "            if not cname:\n",
    "                continue\n",
    "            grouped.setdefault(cname, set())\n",
    "            for opt in cat.get(\"options\", []):\n",
    "                if opt.get(\"enabled\"):\n",
    "                    grouped[cname].add(opt.get(\"name\", \"\").strip())\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for cname in sorted(grouped):\n",
    "            f.write(f\"{cname}\\n\")\n",
    "            for oname in sorted(grouped[cname]):\n",
    "                f.write(f\"- {oname}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def to_csv(df, filename):\n",
    "    '''Save the dataframe to a CSV file.'''\n",
    "    if not df.empty:\n",
    "        df.to_csv(filename, index=False)\n",
    "    else:\n",
    "        print(f\"No data to save to {os.path.basename(filename)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2609db1",
   "metadata": {},
   "source": [
    "### Concatenate all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b6b815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate rows: 765\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = 'sg'\n",
    "OUTPUT_DIR = 'output'\n",
    "TEXT_DIR = 'text'\n",
    "\n",
    "# Read all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(INPUT_DIR, \"*.csv\"))\n",
    "\n",
    "# Clean and combine data\n",
    "dataframes = [clean_data(file) for file in csv_files]\n",
    "pois = combine_dataframes(dataframes)\n",
    "\n",
    "# print(pois.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa468ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "pois = drop_low_reviews(pois, threshold=5)\n",
    "pois = drop_low_ratings(pois, threshold=1.5)\n",
    "pois['price_range'] = pois['price_range'].apply(map_price)\n",
    "pois = pois.rename(columns={\"price_range\": \"price_level\"})\n",
    "# save_categories(pois, filename=os.path.join(TEXT_DIR, 'categories.txt'))\n",
    "# save_about_field(pois, filename=os.path.join(TEXT_DIR, 'about_field.txt'))\n",
    "pois = filter_exclude_categories(pois, exclude_file=os.path.join(TEXT_DIR, 'exclude.txt'))\n",
    "\n",
    "# Handle images & videos\n",
    "pois['images'] = pois['images'].apply(remove_street_view)\n",
    "pois['images'] = pois['images'].apply(clean_images_field)\n",
    "pois['images'] = pois['images'].apply(change_image_resolutions)\n",
    "# pois['videos'] = pois['videos'].apply(clean_videos_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04372f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags\n",
    "pois['kids_friendly'] = pois.apply(update_flag_by_options, axis=1, flag_col='kids_friendly', keywords=['Good for kids'])\n",
    "pois['pets_friendly'] = pois.apply(update_flag_by_options, axis=1, flag_col='pets_friendly', keywords=['Dogs allowed', 'Dogs allowed inside', 'Dogs allowed outside'])\n",
    "pois['wheelchair_rental'] = pois.apply(update_flag_by_options, axis=1, flag_col='wheelchair_rental', keywords=['Wheelchair rental'])\n",
    "pois['wheelchair_accessible_car_park'] = pois.apply(update_flag_by_options, axis=1, flag_col='wheelchair_accessible_car_park', keywords=['Wheelchair accessible car park'])\n",
    "pois['wheelchair_accessible_entrance'] = pois.apply(update_flag_by_options, axis=1, flag_col='wheelchair_accessible_entrance', keywords=['Wheelchair accessible entrance'])\n",
    "pois['wheelchair_accessible_seating'] = pois.apply(update_flag_by_options, axis=1, flag_col='wheelchair_accessible_seating', keywords=['Wheelchair accessible seating'])\n",
    "pois['wheelchair_accessible_toilet'] = pois.apply(update_flag_by_options, axis=1, flag_col='wheelchair_accessible_toilet', keywords=['Wheelchair accessible toilet'])\n",
    "pois['halal_food'] = pois.apply(update_flag_by_options, axis=1, flag_col='halal_food', keywords=['Halal food'])\n",
    "pois['vegan_options'] = pois.apply(update_flag_by_options, axis=1, flag_col='vegan_options', keywords=['Vegan options'])\n",
    "pois['vegetarian_options'] = pois.apply(update_flag_by_options, axis=1, flag_col='vegetarian_options', keywords=['Vegetarian options'])\n",
    "pois['reservations_required'] = pois.apply(update_flag_by_options, axis=1, flag_col='reservations_required', keywords=['Reservations required'])\n",
    "# pois['hiking'] = pois.apply(update_flag_by_options, axis=1, flag_col='hiking', keywords=['Hiking', 'Point-to-point trail', 'Trail difficulty'])\n",
    "# pois['cycling'] = pois.apply(update_flag_by_options, axis=1, flag_col='cycling', keywords=['Cycling'])\n",
    "\n",
    "pois['halal_food'] = pois.apply(update_flag_by_categories, axis=1, flag_col='halal_food', target_categories=['Halal restaurant'])\n",
    "pois['vegetarian_options'] = pois.apply(update_flag_by_categories, axis=1, flag_col='vegetarian_options', target_categories=['Vegetarian restaurant', 'Vegetarian cafe and deli'])\n",
    "pois['vegan_options'] = pois.apply(update_flag_by_categories, axis=1, flag_col='vegan_options', target_categories=['Vegan restaurant'])\n",
    "pois['pets_friendly'] = pois.apply(update_flag_by_categories, axis=1, flag_col='pets_friendly', target_categories=['Cat cafe', 'Dog cafe'])\n",
    "\n",
    "# Remove certain \"about\" field\n",
    "pois['about'] = pois['about'].apply(remove_about, category_name='Atmosphere')\n",
    "pois['about'] = pois['about'].apply(remove_about, category_name='Amenities')\n",
    "pois['about'] = pois['about'].apply(remove_about, category_name='Dining options')\n",
    "pois['about'] = pois['about'].apply(remove_about, category_name='From the business')\n",
    "pois['about'] = pois['about'].apply(remove_about, category_name='Getting here')\n",
    "pois['about'] = pois['about'].apply(remove_about, category_name='Offerings')\n",
    "pois['about'] = pois['about'].apply(remove_about, category_name='Parking')\n",
    "pois['about'] = pois['about'].apply(remove_about, category_name='Payments')\n",
    "pois['about'] = pois['about'].apply(remove_about, category_name='Pets')\n",
    "pois['about'] = pois['about'].apply(remove_about, category_name='Popular for')\n",
    "pois['about'] = pois['about'].apply(remove_about, category_name='Recycling')\n",
    "pois['about'] = pois['about'].apply(remove_about, category_name='Service options')\n",
    "\n",
    "pois['categories'] = pois.apply(deprioritize_category, axis=1, keyword='Tourist attraction')\n",
    "# to_csv(pois, f\"{OUTPUT_DIR}poi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f1d7a",
   "metadata": {},
   "source": [
    "### Michelin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9a3265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare Michelin data\n",
    "michelin = pd.read_csv(os.path.join(OUTPUT_DIR, \"michelin.csv\"))\n",
    "michelin[\"price\"] = michelin[\"price\"].apply(map_price)\n",
    "michelin[\"images\"] = michelin[\"images\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Create lookups\n",
    "michelin_by_phone = michelin.dropna(subset=[\"phone\"]).drop_duplicates(subset=[\"phone\"]).set_index(\"phone\")\n",
    "michelin_by_name = michelin.drop_duplicates(subset=[\"name\"]).set_index(\"name\")\n",
    "\n",
    "# Fill empty price_level\n",
    "def get_price(row):\n",
    "    # Try phone match first\n",
    "    phone = row.get(\"phone\")\n",
    "    if pd.notna(phone) and phone in michelin_by_phone.index:\n",
    "        return michelin_by_phone.loc[phone, \"price\"]\n",
    "    # Fallback: name match (title case to be consistent)\n",
    "    name = row.get(\"name\", \"\").title()\n",
    "    if name in michelin_by_name.index:\n",
    "        return michelin_by_name.loc[name, \"price\"]\n",
    "    return row.get(\"price_level\")\n",
    "\n",
    "pois[\"price_level\"] = pois.apply(get_price, axis=1)\n",
    "\n",
    "# Fill empty description\n",
    "def get_description(row):\n",
    "    phone = row.get(\"phone\")\n",
    "    if pd.notna(phone) and phone in michelin_by_phone.index:\n",
    "        return michelin_by_phone.loc[phone, \"description\"]\n",
    "    name = row.get(\"name\", \"\").title()\n",
    "    if name in michelin_by_name.index:\n",
    "        return michelin_by_name.loc[name, \"description\"]\n",
    "    return row.get(\"descriptions\")\n",
    "\n",
    "pois[\"descriptions\"] = pois.apply(get_description, axis=1)\n",
    "\n",
    "# Merge images\n",
    "def to_list(s):\n",
    "    if isinstance(s, list):\n",
    "        return s\n",
    "    if not s or not isinstance(s, str):\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def merge_images(row):\n",
    "    existing = to_list(row[\"images\"])\n",
    "    phone = row.get(\"phone\")\n",
    "    name = row.get(\"name\", \"\").title()\n",
    "\n",
    "    michelin_imgs = []\n",
    "    if pd.notna(phone) and phone in michelin_by_phone.index:\n",
    "        michelin_imgs = michelin_by_phone.loc[phone, \"images\"]\n",
    "    elif name in michelin_by_name.index:\n",
    "        michelin_imgs = michelin_by_name.loc[name, \"images\"]\n",
    "\n",
    "    return michelin_imgs + existing\n",
    "\n",
    "pois[\"images\"] = pois.apply(merge_images, axis=1)\n",
    "\n",
    "# Save final data\n",
    "to_csv(pois, os.path.join(OUTPUT_DIR, \"poi.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a928de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "TEXT_DIR = Path(\"../text\")\n",
    "ATTRACTIONS_DIR = TEXT_DIR / \"attractions\"\n",
    "ATTRACTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def read_set(path: Path) -> set[str]:\n",
    "    if not path.exists():\n",
    "        return set()\n",
    "    return {\n",
    "        line.strip()\n",
    "        for line in path.read_text(encoding=\"utf-8\").splitlines()\n",
    "        if line.strip()\n",
    "    }\n",
    "\n",
    "def write_set(path: Path, items: set[str]) -> None:\n",
    "    # Sort for deterministic diffs\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(\"\\n\".join(sorted(items)) + (\"\\n\" if items else \"\"), encoding=\"utf-8\")\n",
    "\n",
    "# 1) Load the universe\n",
    "categories = read_set(TEXT_DIR / \"categories.txt\")\n",
    "\n",
    "# 2) Load only selected root files, but all attraction files\n",
    "root_files = {\n",
    "    \"exclude\": TEXT_DIR / \"exclude.txt\",\n",
    "    \"meal\": TEXT_DIR / \"meal.txt\",\n",
    "    \"accommodation\": TEXT_DIR / \"accommodation.txt\",\n",
    "}\n",
    "\n",
    "groups: dict[str, set[str]] = {name: read_set(path) for name, path in root_files.items()}\n",
    "\n",
    "# Load all attraction category files dynamically\n",
    "for p in ATTRACTIONS_DIR.glob(\"*.txt\"):\n",
    "    groups[p.stem] = read_set(p)\n",
    "\n",
    "# 3) Constrain everything to known categories\n",
    "for k in list(groups.keys()):\n",
    "    groups[k] = groups[k] & categories\n",
    "\n",
    "# 4) Special rule for 'family' if present\n",
    "# family := family ∩ categories minus nature and cultural_history\n",
    "if \"family\" in groups:\n",
    "    groups[\"family\"] = groups[\"family\"] - groups.get(\"nature\", set()) - groups.get(\"cultural_history\", set())\n",
    "\n",
    "# 5) Compute filtered union and uniques\n",
    "filter_categories = set().union(*groups.values()) if groups else set()\n",
    "unique = categories - filter_categories\n",
    "\n",
    "# 6) Write outputs\n",
    "write_set(TEXT_DIR / \"unique.txt\", unique)\n",
    "\n",
    "# Persist each group back to its original file path\n",
    "for name, items in groups.items():\n",
    "    if name in root_files:\n",
    "        out_path = root_files[name]\n",
    "    else:\n",
    "        out_path = ATTRACTIONS_DIR / f\"{name}.txt\"\n",
    "    write_set(out_path, items)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
