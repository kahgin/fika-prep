{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7fd1455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "INPUT_DIR = '../sg/'\n",
    "OUTPUT_DIR = '../output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5542c",
   "metadata": {},
   "source": [
    "### Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b1a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filename):\n",
    "    '''Load dataset from the specified filename, clean it by dropping duplicates, filtering for Singapore.'''\n",
    "    df = pd.read_csv(filename, dtype=str)\n",
    "\n",
    "    # Replace unicode\n",
    "    df = df.replace('\\u202f', ' ', regex=True).replace('\\u2013', '-', regex=True)\n",
    "\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(subset=['address'], inplace=True)\n",
    "\n",
    "    # Filter out rows where country is not in SG\n",
    "    df = df[df['complete_address'].astype(str).str.contains('\"country\":\"SG\"', na=False)]\n",
    "\n",
    "    # Drop unused columns\n",
    "    df = drop_columns(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_missing_open_hours(df):\n",
    "    '''Filter out rows where open_hours is not available.'''\n",
    "    return df[df['open_hours'].astype(str) != \"{}\"]\n",
    "\n",
    "def drop_low_ratings(df, threshold=3):\n",
    "    '''Filter out rows where ratings are below a certain threshold.'''\n",
    "    return df[df['rating'].astype(float) >= threshold]\n",
    "\n",
    "def drop_columns(df):\n",
    "    '''Drop unused columns from the dataframe.'''\n",
    "    df.drop(columns=[\n",
    "        'input_id', \n",
    "        'popular_times', \n",
    "        'plus_code',\n",
    "        #  'reviews_per_rating',\n",
    "        'cid',\n",
    "        'status',\n",
    "        'reviews_link',\n",
    "        'thumbnail',\n",
    "        'timezone',\n",
    "        'data_id',\n",
    "        'reservations',\n",
    "        'order_online',\n",
    "        'menu',\n",
    "        'owner',\n",
    "        # 'address',\n",
    "        'user_reviews',\n",
    "        'user_reviews_extended',\n",
    "        'emails',\n",
    "        ], inplace=True, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def combine_dataframes(dfs):\n",
    "    '''Combine multiple dataframes into one, dropping duplicates based on the 'address' column.'''\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    num_duplicates = combined['address'].duplicated().sum()\n",
    "    combined = combined.drop_duplicates(subset=['address'])\n",
    "    print(f\"duplicate rows: {num_duplicates}\")\n",
    "    return combined\n",
    "\n",
    "def print_unique_categories(df, exclude_keyword=None):\n",
    "    \"\"\"\n",
    "    Print unique categories from the dataframe.\n",
    "    Optionally exclude categories containing one or more keywords.\n",
    "    \"\"\"\n",
    "    unique_categories = df['category'].unique()\n",
    "\n",
    "    if exclude_keyword:\n",
    "        exclude_keyword = [kw.lower() for kw in exclude_keyword]\n",
    "        unique_categories = [\n",
    "            c for c in unique_categories\n",
    "            if all(kw not in str(c).lower() for kw in exclude_keyword)\n",
    "        ]\n",
    "\n",
    "    print(\"Unique categories:\")\n",
    "    for category in unique_categories:\n",
    "        print(f\" - {category}\")\n",
    "\n",
    "def to_csv(df, filename):\n",
    "    '''Save the dataframe to a CSV file.'''\n",
    "    if not df.empty:\n",
    "        df.to_csv(filename, index=False)\n",
    "    else:\n",
    "        print(f\"No data to save to {os.path.basename(filename)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221a985",
   "metadata": {},
   "source": [
    "### Food and Beverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9afd8ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate rows: 4608\n"
     ]
    }
   ],
   "source": [
    "cafe = clean_data(f\"{INPUT_DIR}cafe.csv\")\n",
    "restaurant = clean_data(f\"{INPUT_DIR}restaurant.csv\")\n",
    "michelin = clean_data(f\"{INPUT_DIR}michelin.csv\")\n",
    "streetfood = clean_data(f\"{INPUT_DIR}streetfood.csv\")\n",
    "hawker = clean_data(f\"{INPUT_DIR}hawker.csv\")\n",
    "bakery = clean_data(f\"{INPUT_DIR}bakery.csv\")\n",
    "pastry = clean_data(f\"{INPUT_DIR}pastry.csv\")\n",
    "patisserie = clean_data(f\"{INPUT_DIR}patisserie.csv\")\n",
    "ice_cream = clean_data(f\"{INPUT_DIR}ice_cream.csv\")\n",
    "dessert = clean_data(f\"{INPUT_DIR}dessert.csv\")\n",
    "food = combine_dataframes([cafe, restaurant, michelin, streetfood, hawker, bakery, pastry, patisserie, ice_cream, dessert])\n",
    "food = drop_missing_open_hours(food)\n",
    "\n",
    "# Cafe\n",
    "cafe_keywords = ['cafe']\n",
    "cafe = food[food['category'].str.contains('|'.join(cafe_keywords), case=False, na=False)]\n",
    "not_cafe_keywords = ['cafeteria', 'children']\n",
    "cafe = cafe[~cafe['category'].str.contains('|'.join(not_cafe_keywords), case=False, na=False)]\n",
    "to_csv(cafe, f\"{OUTPUT_DIR}cafe.csv\")\n",
    "\n",
    "# Takeaway\n",
    "takeaway_keywords = ['takeaway', 'delivery']\n",
    "takeaways = food[food['category'].str.contains('|'.join(takeaway_keywords), case=False, na=False)]\n",
    "to_csv(takeaways, f\"{OUTPUT_DIR}takeaway.csv\")\n",
    "\n",
    "# Bakery\n",
    "bakery_keywords = ['bakery', 'patisserie', 'pastry']\n",
    "bakery = food[food['category'].str.contains('|'.join(bakery_keywords), case=False, na=False)]\n",
    "not_bakery_keywords = ['wholesale', 'restaurant']\n",
    "bakery = bakery[~bakery['category'].str.contains('|'.join(not_bakery_keywords), case=False, na=False)]\n",
    "to_csv(bakery, f\"{OUTPUT_DIR}bakery.csv\")\n",
    "\n",
    "# Hawker center\n",
    "hawker_keywords = ['hawker']\n",
    "hawker = food[food['category'].str.contains('|'.join(hawker_keywords), case=False, na=False)]\n",
    "to_csv(hawker, f\"{OUTPUT_DIR}hawker_center.csv\")\n",
    "\n",
    "# Food court\n",
    "food_court_keywords = ['food court']\n",
    "food_courts = food[food['category'].str.contains('|'.join(food_court_keywords), case=False, na=False)]\n",
    "to_csv(food_courts, f\"{OUTPUT_DIR}food_court.csv\")\n",
    "\n",
    "# Restaurant\n",
    "restaurant_keywords = ['restaurant', 'diner', 'bistro', 'deli', 'steak', 'grill', 'crab', 'poke', 'gastropub', 'noodle', 'pasta', 'kebab', 'tapas']\n",
    "restaurant = food[food['category'].str.contains('|'.join(restaurant_keywords), case=False, na=False)]\n",
    "not_restaurant_keywords = ['cafe', 'delivery', 'pancake']\n",
    "restaurant = restaurant[~restaurant['category'].str.contains('|'.join(not_restaurant_keywords), case=False, na=False)]\n",
    "to_csv(restaurant, f\"{OUTPUT_DIR}restaurant.csv\")\n",
    "\n",
    "# Dessert\n",
    "dessert_keywords = ['pancake', 'ice cream', 'creperie', 'cake', 'dessert']\n",
    "dessert = food[food['category'].str.contains('|'.join(dessert_keywords), case=False, na=False)]\n",
    "dessert = dessert[~dessert['category'].str.contains('restaurant', case=False, na=False)]\n",
    "to_csv(dessert, f\"{OUTPUT_DIR}dessert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c82babbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee = clean_data(f\"{INPUT_DIR}coffee.csv\")\n",
    "coffee = coffee[coffee['category'].str.contains('coffee', case=False, na=False)]\n",
    "to_csv(coffee, f\"{OUTPUT_DIR}coffee.csv\")\n",
    "\n",
    "bubble_tea = clean_data(f\"{INPUT_DIR}bubble_tea.csv\")\n",
    "bubble_tea = bubble_tea[bubble_tea['category'].str.contains('bubble tea', case=False, na=False)]\n",
    "to_csv(bubble_tea, f\"{OUTPUT_DIR}bubble_tea.csv\")\n",
    "\n",
    "tea = clean_data(f\"{INPUT_DIR}tea.csv\")\n",
    "tea = tea[tea['category'].str.contains('tea', case=False, na=False)]\n",
    "not_tea_keywords = ['bubble tea', 'wholesale', 'manufacture']\n",
    "tea = tea[~tea['category'].str.contains('|'.join(not_tea_keywords), case=False, na=False)]\n",
    "to_csv(tea, f\"{OUTPUT_DIR}tea.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d10d7",
   "metadata": {},
   "source": [
    "### Nightlife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9988e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = clean_data(f\"{INPUT_DIR}bar.csv\")\n",
    "bar = bar[bar['category'].str.contains('bar', case=False, na=False)]\n",
    "to_csv(bar, f\"{OUTPUT_DIR}bar.csv\")\n",
    "\n",
    "pub = clean_data(f\"{INPUT_DIR}pub.csv\")\n",
    "pub = pub[pub['category'].str.contains('pub', case=False, na=False)]\n",
    "pub = pub[~pub['category'].str.contains('gastropub', case=False, na=False)]\n",
    "to_csv(pub, f\"{OUTPUT_DIR}pub.csv\")\n",
    "\n",
    "club = clean_data(f\"{INPUT_DIR}club.csv\")\n",
    "club = club[club['category'].str.contains('club', case=False, na=False)]\n",
    "not_club_keywords = ['sauna', 'social', 'country', 'polo']\n",
    "club = club[~club['category'].str.contains('|'.join(not_club_keywords), case=False, na=False)]\n",
    "to_csv(club, f\"{OUTPUT_DIR}club.csv\")\n",
    "\n",
    "brewery = clean_data(f\"{INPUT_DIR}brewery.csv\")\n",
    "brewery_keywords = ['brewery', 'beer garden']\n",
    "brewery = brewery[brewery['category'].str.contains('|'.join(brewery_keywords), case=False, na=False)]\n",
    "to_csv(brewery, f\"{OUTPUT_DIR}brewery.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e85148",
   "metadata": {},
   "source": [
    "### Religious sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7130fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_of_worship = clean_data(f'{INPUT_DIR}place_of_worship.csv')\n",
    "place_of_worship = drop_missing_open_hours(place_of_worship)\n",
    "\n",
    "church = place_of_worship[place_of_worship['category'].str.contains('church', case=False, na=False)]\n",
    "to_csv(church, f\"{OUTPUT_DIR}church.csv\")\n",
    "temple = place_of_worship[place_of_worship['category'].str.contains('temple', case=False, na=False)]\n",
    "to_csv(temple, f\"{OUTPUT_DIR}temple.csv\")\n",
    "mosque = place_of_worship[place_of_worship['category'].str.contains('mosque', case=False, na=False)]\n",
    "to_csv(mosque, f\"{OUTPUT_DIR}mosque.csv\")\n",
    "cathedral = place_of_worship[place_of_worship['category'].str.contains('cathedral', case=False, na=False)]\n",
    "to_csv(cathedral, f\"{OUTPUT_DIR}cathedral.csv\")\n",
    "synagogue = place_of_worship[place_of_worship['category'].str.contains('synagogue', case=False, na=False)]\n",
    "to_csv(synagogue, f\"{OUTPUT_DIR}synagogue.csv\")\n",
    "gurudwara = place_of_worship[place_of_worship['category'].str.contains('gurudwara', case=False, na=False)]\n",
    "to_csv(gurudwara, f\"{OUTPUT_DIR}gurudwara.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3916239",
   "metadata": {},
   "source": [
    "### Family Attractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "271599c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate rows: 8\n"
     ]
    }
   ],
   "source": [
    "zoo = clean_data(f\"{INPUT_DIR}zoo.csv\") # aquarium is scraped too\n",
    "theme_park = clean_data(f\"{INPUT_DIR}theme_park.csv\")\n",
    "wildlife = clean_data(f\"{INPUT_DIR}wildlife.csv\")\n",
    "family_attraction = combine_dataframes([zoo, theme_park, wildlife])\n",
    "family_attraction = drop_missing_open_hours(family_attraction)\n",
    "\n",
    "zoo = family_attraction[family_attraction['category'].str.contains('zoo', case=False, na=False)]\n",
    "to_csv(zoo, f\"{OUTPUT_DIR}zoo.csv\")\n",
    "\n",
    "aquarium = family_attraction[family_attraction['category'].str.contains('aquarium', case=False, na=False)]\n",
    "to_csv(aquarium, f\"{OUTPUT_DIR}aquarium.csv\")\n",
    "\n",
    "theme_park_keywords = ['theme', 'amusement', 'water']\n",
    "theme_park = family_attraction[family_attraction['category'].str.contains('|'.join(theme_park_keywords), case=False, na=False)]\n",
    "to_csv(theme_park, f\"{OUTPUT_DIR}theme_park.csv\")\n",
    "\n",
    "wildlife_park = family_attraction[family_attraction['category'].str.contains('wildlife', case=False, na=False)]\n",
    "to_csv(wildlife_park, f\"{OUTPUT_DIR}wildlife_park.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e14826",
   "metadata": {},
   "source": [
    "### Shopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c8b7e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate rows: 391\n"
     ]
    }
   ],
   "source": [
    "mall = clean_data(f'{INPUT_DIR}mall.csv')\n",
    "market = clean_data(f'{INPUT_DIR}market.csv')\n",
    "store = clean_data(f'{INPUT_DIR}store.csv')\n",
    "vintage = clean_data(f'{INPUT_DIR}vintage.csv')\n",
    "souvenir_shop = clean_data(f'{INPUT_DIR}souvenir_shop.csv')\n",
    "shopping = combine_dataframes([mall, market, store, vintage, souvenir_shop])\n",
    "shopping = drop_missing_open_hours(shopping)\n",
    "\n",
    "mall = shopping[shopping['category'].str.contains('mall', case=False, na=False)]\n",
    "mall.to_csv(f'{OUTPUT_DIR}mall.csv', index=False)\n",
    "\n",
    "souvenir_keywords = ['souvenir store', 'gift shop']\n",
    "souvenir_shop = souvenir_shop[souvenir_shop['category'].str.contains('|'.join(souvenir_keywords), case=False, na=False)]\n",
    "souvenir_shop.to_csv(f'{OUTPUT_DIR}souvenir_shop.csv', index=False)\n",
    "\n",
    "clothing_store = shopping[shopping['category'].str.contains('clothing store', case=False, na=False)]\n",
    "clothing_store.to_csv(f'{OUTPUT_DIR}clothing_store.csv', index=False)\n",
    "\n",
    "market = shopping[shopping['category'].str.contains('market', case=False, na=False)]\n",
    "market.to_csv(f'{OUTPUT_DIR}market.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cffd7cd",
   "metadata": {},
   "source": [
    "### Culture & History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "684b44b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate rows: 13\n"
     ]
    }
   ],
   "source": [
    "museums = clean_data(f'{INPUT_DIR}museum.csv')\n",
    "gallery = clean_data(f'{INPUT_DIR}gallery.csv')\n",
    "historical_landmark = clean_data(f'{INPUT_DIR}historical_landmark.csv')\n",
    "visitor_center = clean_data(f'{INPUT_DIR}visitor_center.csv')\n",
    "culture_history = combine_dataframes([museums, gallery, historical_landmark, visitor_center])\n",
    "\n",
    "historical_landmark = culture_history[culture_history['category'].str.contains('historical landmark', case=False, na=False)]\n",
    "to_csv(historical_landmark, f'{OUTPUT_DIR}historical_landmark.csv')\n",
    "\n",
    "museum = culture_history[culture_history['category'].str.contains('museum', case=False, na=False)]\n",
    "to_csv(museum, f'{OUTPUT_DIR}museum.csv')\n",
    "\n",
    "gallery = culture_history[culture_history['category'].str.contains('gallery', case=False, na=False)]\n",
    "to_csv(gallery, f'{OUTPUT_DIR}gallery.csv')\n",
    "\n",
    "visitor_center = culture_history[culture_history['category'].str.contains('visitor center', case=False, na=False)]\n",
    "to_csv(visitor_center, f'{OUTPUT_DIR}visitor_center.csv')\n",
    "\n",
    "heritage = culture_history[culture_history['category'].str.contains('heritage', case=False, na=False)]\n",
    "heritage = heritage[~heritage['category'].str.contains('museum', case=False, na=False)]\n",
    "to_csv(heritage, f'{OUTPUT_DIR}heritage.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b673b",
   "metadata": {},
   "source": [
    "### Nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a55967d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate rows: 1142\n",
      "No data to save to waterfall.csv.\n"
     ]
    }
   ],
   "source": [
    "beach = clean_data(f'{INPUT_DIR}beach.csv')\n",
    "waterfall = clean_data(f'{INPUT_DIR}waterfall.csv')\n",
    "park = clean_data(f'{INPUT_DIR}park.csv')\n",
    "garden = clean_data(f'{INPUT_DIR}garden.csv')\n",
    "nature = combine_dataframes([beach, waterfall, park, garden])\n",
    "\n",
    "beach = nature[nature['category'].str.contains('beach', case=False, na=False)]\n",
    "to_csv(beach, f'{OUTPUT_DIR}beach.csv')\n",
    "\n",
    "waterfall = nature[nature['category'].str.contains('waterfall', case=False, na=False)]\n",
    "to_csv(waterfall, f'{OUTPUT_DIR}waterfall.csv')\n",
    "\n",
    "park = nature[nature['category'].str.contains('park', case=False, na=False)]\n",
    "to_csv(park, f'{OUTPUT_DIR}park.csv')\n",
    "\n",
    "garden = nature[nature['category'].str.contains('garden', case=False, na=False)]\n",
    "to_csv(garden, f'{OUTPUT_DIR}garden.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4273b5e",
   "metadata": {},
   "source": [
    "### Sightseeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f39cafe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate rows: 44\n"
     ]
    }
   ],
   "source": [
    "scenic_spot = clean_data(f'{INPUT_DIR}scenic_spot.csv')\n",
    "tourist_attraction = clean_data(f'{INPUT_DIR}tourist_attraction.csv')\n",
    "sightseeing = combine_dataframes([scenic_spot, tourist_attraction])\n",
    "\n",
    "scenic_keywords = ['scenic', 'viewpoint', 'panorama', 'panoramic', 'observation', 'hilltop']\n",
    "scenic_spot = sightseeing[\n",
    "    (sightseeing['category'].str.contains('|'.join(scenic_keywords), case=False, na=False)) |\n",
    "    (sightseeing['descriptions'].str.contains('|'.join(scenic_keywords), case=False, na=False))\n",
    "]\n",
    "to_csv(scenic_spot, f'{OUTPUT_DIR}scenic_spot.csv')\n",
    "\n",
    "tourist_attraction = tourist_attraction[tourist_attraction['category'].str.contains('tourist attraction', case=False, na=False)]\n",
    "to_csv(tourist_attraction, f'{OUTPUT_DIR}tourist_attraction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffea2bf",
   "metadata": {},
   "source": [
    "### Art & Craft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "854a9dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "workshop = clean_data(f\"{INPUT_DIR}art_craft.csv\")\n",
    "\n",
    "def drop_not_on_site_service(df):\n",
    "    '''Filter out rows where the 'about' column does not mention on-site services.'''\n",
    "    return df[df['about'].str.contains('\"name\":\"on-site services\",\"enabled\":true', case=False, na=False)]\n",
    "\n",
    "workshop = drop_missing_open_hours(workshop)\n",
    "workshop = drop_not_on_site_service(workshop)\n",
    "\n",
    "# print_unique_categories(workshop)\n",
    "workshop_keywords = ['studio', 'handicraft', 'class', 'store', 'art', 'pottery', 'candle', 'leather']\n",
    "workshop = workshop[workshop['category'].str.contains('|'.join(workshop_keywords), case=False, na=False)]\n",
    "to_csv(workshop, f\"{OUTPUT_DIR}art_craft.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2802322",
   "metadata": {},
   "source": [
    "### Adventure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0e228e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiking_area = clean_data(f\"{INPUT_DIR}hiking_area.csv\")\n",
    "hiking_area = drop_missing_open_hours(hiking_area)\n",
    "hiking_area = hiking_area[hiking_area['category'].str.contains('hiking area', case=False, na=False)]\n",
    "to_csv(hiking_area, f\"{OUTPUT_DIR}hiking_area.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5961d08",
   "metadata": {},
   "source": [
    "### Relaxation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47608581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate rows: 31\n"
     ]
    }
   ],
   "source": [
    "spa = clean_data(f\"{INPUT_DIR}spa.csv\")\n",
    "sauna = clean_data(f\"{INPUT_DIR}sauna.csv\")\n",
    "relaxation = combine_dataframes([spa, sauna])\n",
    "relaxation = drop_missing_open_hours(relaxation)\n",
    "\n",
    "spa = relaxation[relaxation['category'].str.contains('spa', case=False, na=False)]\n",
    "to_csv(spa, f\"{OUTPUT_DIR}spa.csv\")\n",
    "\n",
    "sauna = relaxation[relaxation['category'].str.contains('sauna', case=False, na=False)]\n",
    "to_csv(sauna, f\"{OUTPUT_DIR}sauna.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df200d",
   "metadata": {},
   "source": [
    "### Accommodation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2429ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel = clean_data(f\"{INPUT_DIR}hotel.csv\")\n",
    "to_csv(hotel, f\"{OUTPUT_DIR}hotel.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
